from typing import Iterable, Dict, Union

import torch
from torch.utils.data import Dataset
from tqdm import tqdm

from bienc.tokenizer import tokenize_cross


class CrossDataset(Dataset):
    """Dataset with candidates to be used for crossencoder training."""
    def __init__(self,
                 topic_ids: Iterable[str],
                 topic_gold_ids: Iterable[str],
                 topic_cand_ids: Iterable[str],
                 topic2text: Dict[str, str], content2text: Dict[str, str],
                 num_tokens: int,
                 num_cands: int,
                 is_val: bool):
        """
        Training dataset for the bi-encoder embedding content and topic texts into the same space.
        :param topic_ids: iterable over topic ids
        :param topic_gold_ids: iterable over concatenated gold content ids.
        :param topic_cand_ids: iterable over concatenated candidate ids (both true and false positive generated by biencoder)
        :param topic2text: dictionary mapping topic to its text representation
        :param content2text: dictionary mapping content to its text representation
        :param num_tokens: how many tokens to use for joint representation
        :param num_cands: how many candidates to consider
        :param is_val: whether this is a validation dataset. in that case, only include positives if they are candidates
        """
        self.topic_ids = []
        self.content_ids = []
        self.labels = []
        self.topic2text = topic2text
        self.content2text = content2text
        self.num_tokens = num_tokens
        self.num_cands = num_cands

        for topic_id, cat_gold_ids, cat_cand_ids, in tqdm(zip(topic_ids, topic_gold_ids, topic_cand_ids)):
            gold_ids = set(cat_gold_ids.split())
            cand_ids = set(cat_cand_ids.split()[:num_cands])
            if is_val:
                positive_ids = sorted(list(cand_ids & gold_ids))
            else:
                positive_ids = sorted(list(gold_ids))
            negative_ids = sorted(list(cand_ids - gold_ids))
            for content_id in positive_ids:
                self.topic_ids.append(topic_id)
                self.content_ids.append(content_id)
                self.labels.append(1)
            for content_id in negative_ids:
                self.topic_ids.append(topic_id)
                self.content_ids.append(content_id)
                self.labels.append(0)

    def __getitem__(self, idx):
        enc = tokenize_cross(self.topic2text[self.topic_ids[idx]], self.content2text[self.content_ids[idx]], self.num_tokens)
        return torch.tensor(enc["input_ids"]), torch.tensor(enc["attention_mask"]), self.labels[idx]

    def __len__(self):
        return len(self.topic_ids)


class CrossInferenceDataset(Dataset):
    """Dataset for crossencoder inference."""
    def __init__(self,
                 topic_ids: Iterable[str],
                 topic_cand_ids: Iterable[str],
                 topic2text: Dict[str, str], content2text: Dict[str, str],
                 num_tokens: int):
        """
        Training dataset for the bi-encoder embedding content and topic texts into the same space.
        :param topic_ids: iterable over topic ids
        :param topic_cand_ids: iterable over concatenated candidate ids (both true and false positive generated by biencoder)
        :param topic2text: dictionary mapping topic to its text representation
        :param content2text: dictionary mapping content to its text representation
        :param num_tokens: how many tokens to use for joint representation
        """
        self.topic_ids = []
        self.content_ids = []
        self.labels = []
        self.topic2text = topic2text
        self.content2text = content2text
        self.num_tokens = num_tokens

        for topic_id, cat_cand_ids, in tqdm(zip(topic_ids, topic_cand_ids)):
            cand_ids = set(cat_cand_ids.split())
            for content_id in cand_ids:
                self.topic_ids.append(topic_id)
                self.content_ids.append(content_id)

    def __getitem__(self, idx):
        enc = tokenize_cross(self.topic2text[self.topic_ids[idx]], self.content2text[self.content_ids[idx]], self.num_tokens)
        return torch.tensor(enc["input_ids"]), torch.tensor(enc["attention_mask"])

    def __len__(self):
        return len(self.topic_ids)
