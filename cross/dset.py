from typing import Iterable, Dict

import numpy as np
import torch
from torch.utils.data import Dataset
from tqdm import tqdm

from cross.tokenizer import tokenize_cross
import cross.tokenizer as tk


class CrossDataset(Dataset):
    """Dataset with candidates to be used for crossencoder training."""
    def __init__(self,
                 topic_ids: Iterable[str],
                 topic_gold_ids: Iterable[str],
                 topic_cand_ids: Iterable[str],
                 topic2text: Dict[str, str], content2text: Dict[str, str],
                 num_tokens: int,
                 is_val: bool,
                 dropout: float = 0.0):
        """
        Training dataset for the bi-encoder embedding content and topic texts into the same space.
        :param topic_ids: iterable over topic ids
        :param topic_gold_ids: iterable over concatenated gold content ids.
        :param topic_cand_ids: iterable over concatenated candidate ids (both true and false positive generated by biencoder)
        :param topic2text: dictionary mapping topic to its text representation
        :param content2text: dictionary mapping content to its text representation
        :param num_tokens: how many tokens to use for joint representation
        :param is_val: whether this is a validation dataset. in that case, only include positives if they are candidates
        :param dropout: randomly mask tokens at this rate
        """
        self.topic_ids = []
        self.content_ids = []
        self.labels = []
        self.topic2text = topic2text
        self.content2text = content2text
        self.num_tokens = num_tokens
        self.dropout = dropout

        for topic_id, cat_gold_ids, cat_cand_ids, in tqdm(zip(topic_ids, topic_gold_ids, topic_cand_ids)):
            gold_ids = set(cat_gold_ids.split())
            cand_ids = set(cat_cand_ids.split())
            if is_val:
                positive_ids = sorted(list(cand_ids & gold_ids))
            else:
                positive_ids = sorted(list(gold_ids))
            negative_ids = sorted(list(cand_ids - gold_ids))
            for content_id in positive_ids:
                self.topic_ids.append(topic_id)
                self.content_ids.append(content_id)
                self.labels.append(1)
            for content_id in negative_ids:
                self.topic_ids.append(topic_id)
                self.content_ids.append(content_id)
                self.labels.append(0)

    def __getitem__(self, idx):
        input_ids, attention_mask, special_tokens_mask = tokenize_cross(self.topic2text[self.topic_ids[idx]], self.content2text[self.content_ids[idx]], self.num_tokens)
        if self.dropout:
            r = torch.rand(len(input_ids))
            input_ids[(special_tokens_mask == 0) & (r < self.dropout)] = tk.MASK_TOKEN_ID
        return input_ids, attention_mask, torch.tensor(self.labels[idx])

    def __len__(self):
        return len(self.topic_ids)


class CrossInferenceDataset(Dataset):
    """Dataset for crossencoder inference."""
    def __init__(self,
                 topic_ids: Iterable[str],
                 topic_cand_ids: Iterable[str],
                 topic2text: Dict[str, str], content2text: Dict[str, str],
                 num_tokens: int):
        """
        Training dataset for the bi-encoder embedding content and topic texts into the same space.
        :param topic_ids: iterable over topic ids
        :param topic_cand_ids: iterable over concatenated candidate ids (both true and false positive generated by biencoder)
        :param topic2text: dictionary mapping topic to its text representation
        :param content2text: dictionary mapping content to its text representation
        :param num_tokens: how many tokens to use for joint representation
        """
        self.topic_ids = []
        self.content_ids = []
        self.topic2text = topic2text
        self.content2text = content2text
        self.num_tokens = num_tokens

        for topic_id, cat_cand_ids, in tqdm(zip(topic_ids, topic_cand_ids)):
            cand_ids = cat_cand_ids.split()
            assert len(set(cand_ids)) == len(cand_ids)
            for content_id in cand_ids:
                self.topic_ids.append(topic_id)
                self.content_ids.append(content_id)

    def __getitem__(self, idx):
        input_ids, attention_mask, _ = tokenize_cross(self.topic2text[self.topic_ids[idx]], self.content2text[self.content_ids[idx]], self.num_tokens)
        return input_ids, attention_mask

    def __len__(self):
        return len(self.topic_ids)
